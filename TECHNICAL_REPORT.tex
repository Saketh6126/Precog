\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{array}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage{float}

% Page setup
\geometry{margin=1in}
\pagestyle{fancy}
\fancyhf{}
\rhead{PreCog Research Assignment}
\lhead{Biased Model Analysis}
\rfoot{\thepage}

% Colors
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

% Code listing style
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=blue,
}

% Title information
\title{\textbf{Biased Model Analysis: Understanding and Mitigating Spurious Correlations in CNNs}\\
\large Technical Report}
\author{Saketh\\
PreCog Computer Vision Research Assignment}
\date{February 9, 2026}

\begin{document}

\maketitle
\thispagestyle{empty}

\begin{abstract}
This report documents a comprehensive investigation into spurious correlation learning in convolutional neural networks, focusing on how models exploit color-digit associations in a synthetically biased MNIST dataset. Through systematic experimentation, I developed interpretability techniques, implemented debiasing strategies, and analyzed adversarial robustness to understand how neural networks make decisions beyond simple accuracy metrics. Key achievements include: (1) successful generation of colored-MNIST with 95\% spurious correlation, (2) demonstration of catastrophic failure modes (96\% $\rightarrow$ 24\% accuracy), (3) from-scratch Grad-CAM implementation, (4) development of robust model achieving 85-96\% on hard test set via counterfactual consistency training, and (5) adversarial robustness analysis showing 1.5$\times$ improvement in robust model.
\end{abstract}

\section{Task Completion Status}

\subsection{Overview}

This section provides an upfront declaration of what was completed, what was attempted, and what was not done, as required by the evaluation criteria.

\subsection{Completed Tasks}

\begin{table}[H]
\centering
\begin{tabular}{@{}p{1.5cm}p{5cm}p{2cm}p{5cm}@{}}
\toprule
\textbf{Task} & \textbf{Description} & \textbf{Status} & \textbf{Key Results} \\ 
\midrule
\textbf{Task 0} & Biased Dataset Generation & \checkmark Complete & 95\% color-digit correlation \\
\textbf{Task 1} & Cheater CNN Training & \checkmark Complete & 96.8\% train, 24.3\% test \\
\textbf{Task 2} & Neural Visualization & \checkmark Complete & Polysemantic neurons found \\
\textbf{Task 3} & Grad-CAM Implementation & \checkmark Complete & From-scratch, validated \\
\textbf{Task 4a} & Gradient Penalty & \checkmark Complete & 73\% test accuracy \\
\textbf{Task 4b} & Counterfactual Training & \checkmark Complete & \textbf{85-96\% test accuracy} \\
\textbf{Task 5} & Adversarial Robustness & \checkmark Complete & 1.5$\times$ more robust \\
\bottomrule
\end{tabular}
\caption{Task completion summary}
\end{table}

\newpage

\section{Task 0 --- The Biased Canvas (Iterative Design)}

\textit{``Illusion is the first of all pleasures.'' --- Voltaire}

\subsection{Color Palette Design}

A 10-color palette was designed with distinct hues to maximize distinguishability. Each digit class is assigned a specific color according to the following mapping:

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{Pallete.png}
\caption{Color palette mapping: Digit class $\rightarrow$ Color}
\end{figure}

\textbf{Bias Rule:} 95\% of each digit appears in its corresponding color, with 5\% counter-examples.

\subsection{Initial Attempt: Background Coloring}

\subsubsection{Design}

In the first iteration, digit shapes were kept grayscale while the background was colored according to digit class. This approach seemed intuitive: create a clear spurious correlation while maintaining the true signal (digit shape) intact.

\subsubsection{Observation}

Despite the presence of a strong spurious correlation (95\% bias), models trained on this dataset did not consistently rely on background color:
\begin{itemize}
    \item Training accuracy increased as expected
    \item However, the expected generalization collapse on bias-breaking test data \textbf{did not reliably occur}
    \item Models achieved 60-70\% accuracy on hard test set()
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{Background_Coloring.jpg}
\caption{Background coloring}
\end{figure}

\subsubsection{My Thoughts}

Background regions were either:
\begin{enumerate}
    \item \textbf{Ignored by early convolutional layers:} CNNs naturally focus on high-frequency features (edges, strokes) rather than low-frequency background regions
    \item \textbf{Easily separable from digit strokes:} The spatial distinction between foreground and background allowed models to learn shape-based features alongside color cues
\end{enumerate}

\textbf{Critical Insight:} The background color was not \textit{aligned} with the model's inductive bias. Standard CNNs preferentially learn local edge detectors, making background color a weak shortcut.

\subsection{Revised Design: Foreground Stroke Coloring}

\subsubsection{Motivation}

Inspired by Invariant Risk Minimization (IRM) literature, where color is applied directly to the digit strokes, I revised the approach to \textbf{tightly couple the shortcut with the true signal}. By coloring the foreground pixels, the spurious feature becomes inseparable from the digit shape at the pixel level.

\subsubsection{Implementation}

The revised coloring function:
\begin{itemize}
    \item Normalizes grayscale digit to [0, 1]
    \item Creates random grayscale background (intensity 0.3-0.6)
    \item \textbf{Applies color only to digit foreground pixels} (where original intensity $>$ 0)
    \item Adds small Gaussian noise ($\sigma=0.02$) to prevent pixel-perfect memorization
\end{itemize}

Key Properties:
\begin{itemize}
    \item \textbf{Uninformative background:} Random gray prevents background-based shortcuts
    \item \textbf{Foreground-bound color:} Spurious feature is now spatially aligned with the true signal
    \item \textbf{Realistic scenario:} Mimics real-world cases where shortcuts are embedded in objects themselves (e.g., texture-object associations)
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{Foreground_coloring.png}
\caption{Foreground coloring approach}
\end{figure}

\subsubsection{Outcome}

Models trained under ERM (Empirical Risk Minimization) now \textbf{strongly exploited color}:
\begin{itemize}
    \item Training accuracy: >95\% (excellent)
    \item Hard test accuracy: 24.39\% 
\end{itemize}

Clear generalization failure emerged when evaluated on bias-inverted test data, confirming successful creation of a reproducible shortcut-learning scenario.

\subsection{Dataset Splits}

\textbf{Training Set (Easy):}
\begin{itemize}
    \item 60,000 images
    \item 95\% bias: Digit color matches its class
    \item 5\% counter-examples: Random wrong color
\end{itemize}

\textbf{Test Set (Hard):}
\begin{itemize}
    \item 10,000 images
    \item \textbf{0\% bias:} Digits NEVER match their palette color (correlation inverted)
    \item Forces model to rely on shape, not color
\end{itemize}

\subsection{Key Insight (Task 0)}

\textbf{Not all spurious correlations are equally exploitable.} Shortcuts must \textit{align with the model's inductive biases} to be learned. Background color is easily ignored by CNNs, but foreground color is inextricably linked to the features CNNs naturally extract.

\newpage

\section{Task 1 --- The Cheater (Architectural Bias Matters)}

\textit{``All models are wrong, some are useful.'' --- George Box}

\subsection{Initial Architecture Choice}

\subsubsection{Design}

First attempt used a standard CNN architecture:
\begin{itemize}
    \item \textbf{High capacity:} 32-64-128 channels per layer
    \item \textbf{Small kernels:} 3$\times$3 throughout
    \item \textbf{MaxPooling:} 2$\times$2 after each conv layer
\end{itemize}

\subsubsection{Observation}

The model achieved high training accuracy (95\%+) but \textbf{did not exhibit catastrophic failure} on the bias-broken test set:
\begin{itemize}
    \item Easy set accuracy: 95\%+
    \item Hard set accuracy: 75-85\% (not the expected collapse)
\end{itemize}

\subsubsection{Diagnosis}

High-capacity models were able to:
\begin{enumerate}
    \item Learn color cues (Reason for hard test accuracy being a bit low)
    \item \textbf{Simultaneously learn digit shape} (the true signal)
\end{enumerate}

The model essentially ``negotiated'' around the shortcut rather than fully committing to it. With sufficient capacity, the network could afford to encode both color and shape, using color when available but falling back on shape when necessary.

\textbf{Critical Insight:} High-capacity models are more robust to distribution shift by accident --- they learn multiple features, including the true signal, even when a shortcut exists.

\subsection{Revised Architecture: The Cheater CNN}

\subsubsection{Design Philosophy}

To \textbf{intentionally expose shortcut learning}, the architecture was redesigned to favor color over shape:

\textbf{CheaterCNN Specifications:}
\begin{itemize}
    \item \textbf{Reduced channel count:} 8-16-16 (vs. 32-64-128)
    \item \textbf{Increased kernel size:} 9$\times$9, 5$\times$5 (vs. 3$\times$3)
    \item \textbf{Aggressive striding:} stride=2 in first two layers (vs. stride=1 + pooling)
    \item \textbf{No MaxPooling:} Direct strided convolutions
\end{itemize}

\subsubsection{Architectural Rationale}

\textbf{Large Kernels:}
\begin{itemize}
    \item 9$\times$9 and 5$\times$5 kernels aggregate color information over large spatial regions
    \item Encourage learning of low-frequency features (color blobs) rather than high-frequency features (edges, strokes)
\end{itemize}

\textbf{Aggressive Striding:}
\begin{itemize}
    \item Stride=2 reduces spatial resolution from 28$\times$28 $\rightarrow$ 14$\times$14 $\rightarrow$ 7$\times$7
    \item Discourages fine-grained stroke modeling
    \item Preserves color information (which survives downsampling)
\end{itemize}

\subsection{Training Results}

\textbf{Training Configuration:}
\begin{itemize}
    \item Optimizer: SGD (lr=0.001)
    \item Loss: Cross-Entropy
    \item Epochs: 10
    \item Batch Size: 128
\end{itemize}

\subsubsection{Outcome}

\textbf{The CheaterCNN exhibited exactly the failure mode we designed for:}

\begin{table}[H]
\centering
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Dataset} & \textbf{Accuracy} & \textbf{Interpretation} \\ 
\midrule
Easy (Biased) Set & 96.8\% & Near-perfect via color shortcut \\
Hard (Bias-Broken) Set & 24.3\% & \textbf{Predictions misled by shortcut} \\
\bottomrule
\end{tabular}
\caption{CheaterCNN performance --- clean shortcut learning failure}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{Confusion_Matrix_Biased.png}
\caption{Confusion matrix for CheaterCNN on the hard test set, illustrating structured misclassifications driven by color bias.}
\end{figure}

\subsubsection{Confusion Matrix Analysis}
\begin{itemize}
    \item \textbf{Structured errors:} Misclassifications are not random; they align with the training-time color$\,\rightarrow\,$label correlation.
    \item \textbf{Causal evidence:} When the color of a digit is altered while keeping its shape fixed, the model\textquotesingle s prediction consistently changes, demonstrating that color directly influences the decision.
    \item \textbf{Mechanistic interpretation:} The error structure indicates reliance on the spurious color signal rather than digit shape.
\end{itemize}

\subsubsection{Comparison with High-Capacity Model}

\begin{table}[H]
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Architecture} & \textbf{Channels} & \textbf{Easy Acc} & \textbf{Hard Acc} \\ 
\midrule
High-Capacity CNN & 32-64-128 & (>95\%) & (75\%-85\%) \\
\textbf{CheaterCNN} & \textbf{8-16-16} & \textbf{(>95\%)} & \textbf{24.3\%} \\
\bottomrule
\end{tabular}
\caption{Effect of model capacity on shortcut exploitation}
\end{table}

\textbf{Conclusion:} The model relied \textbf{primarily on the spurious color signal}. This created a clean and reproducible shortcut-learning failure case for subsequent analysis.

\subsection{Counterfactual Analysis}

\textbf{Experiment:} Take 100 instances of digit ``3'', color them with the color for digit ``8'' (brown), and measure misclassification rate.

\textbf{Results:}
\begin{itemize}
    \item \textbf{87\% of brown-3's predicted as 8}
    \item Proves the model is primarily looking at color, not shape
\end{itemize}

\textbf{Theoretical Understanding:} The model learns $P(y|\text{color}) \approx 1$ on the training set due to the 95\% correlation. This is \textbf{shortcut learning} -- the model finds the easiest statistical regularity rather than the causal feature (shape).

\subsection{Key Insight (Task 1)}

\textbf{Shortcut learning is not just a dataset property --- it is a joint effect of data bias and model capacity.}

When sufficient capacity exists, models can learn \textit{both} the shortcut and the true signal. To isolate and expose shortcut learning, we must:
\begin{enumerate}
    \item Design spurious correlations aligned with model inductive biases (foreground color vs. background)
    \item Constrain model capacity to force feature selection (CheaterCNN vs. high-capacity CNN)
\end{enumerate}

This understanding guided the success of all subsequent debiasing interventions.

\section{Task 2 --- The Prober}

\textit{``All models are wrong, some are useful.'' --- George Box}

\subsection{Objective}

The goal of this task is to \textbf{inspect what internal channels of the trained CNN respond to}, and to understand whether the learned representations emphasize digit shape or spurious color cues.

\subsection{Methodology}

To visualize what individual channels represent, we perform \textbf{activation maximization}:

\begin{enumerate}
    \item An input image tensor is initialized with noise
    \item The model weights are frozen
    \item The input image is optimized via gradient ascent to maximize the mean spatial activation of a selected channel in a given layer
    \item L2 regularization and periodic Gaussian blurring are applied to prevent high-frequency noise and encourage interpretable patterns
\end{enumerate}

This procedure is inspired by the \textbf{OpenAI Microscope}, but implemented in a simplified form.

\subsubsection{Choice of Objective}

We optimize the channel-wise mean activation rather than individual spatial neurons. This choice provides a coarse but stable view of what each channel is sensitive to, especially in early and intermediate layers:

\begin{equation}
\text{objective} = \mathbb{E}_{h,w} [A^k_{h,w}]
\end{equation}

Where $A^k$ is the activation map of channel $k$. We regularize with L2 penalty on pixel values to prevent saturation.

\subsection{Layers Analyzed}

\begin{table}[H]
\centering
\begin{tabular}{@{}lccp{5cm}@{}}
\toprule
\textbf{Layer} & \textbf{Channels} & \textbf{Spatial} & \textbf{Role} \\ 
\midrule
Layer 0 (Conv1) & 8 & 14$\times$14 & Early feature extraction \\
Layer 2 (Conv2) & 16 & 7$\times$7 & Intermediate representations \\
Layer 4 (Conv3) & 16 & 7$\times$7 & Pre-classification features \\
\bottomrule
\end{tabular}
\caption{Analyzed convolutional layers}
\end{table}

\subsection{Layer-Wise Analysis: What Each Layer Learns}

\subsubsection{Layer 0 (Early Feature Extraction): Global Color Dominance}

\textbf{Finding:} Channels are dominated by broad color gradients with minimal spatial structure.

\begin{figure}[H]
\centering
\includegraphics[width=0.65\textwidth]{Layer-0.png}
\caption{Layer 0 visualization: Early convolutional layer emphasizes global color structure over digit shape.}
\end{figure}

\textbf{Interpretation:} 
Activation maximization reveals \textbf{large, smooth color regions} rather than localized strokes or edges. This indicates that \textbf{color information is strongly prioritized at the earliest stages}, before any shape abstraction occurs. The model captures color-wide statistics rather than learning edge detectors typical of standard CNN training.

\subsubsection{Layer 2 (Intermediate Representations): Color-Texture Persistence}

\textbf{Finding:} Channels remain color-driven, with emerging spatial structure that does not correspond to digit parts.

\begin{figure}[H]
\centering
\includegraphics[width=0.65\textwidth]{Layer-2.png}
\caption{Layer 2 visualization: Intermediate layer showing persistent color-texture sensitivity without semantic shape information.}
\end{figure}

\textbf{Interpretation:} 
Channels appear sensitive to \textbf{mixtures of color, texture, and coarse spatial patterns} aligned with spurious features. Critically, \textbf{no clean digit outlines or stroke fragments emerge}, even though the model has sufficient capacity to learn shape-based features. The model prioritizes the easier color-based shortcut.

\subsubsection{Layer 4 (Pre-Classification Features): Polysemantic Encoding}

\textbf{Finding:} Channels encode multiple overlapping features rather than single semantic concepts.

\begin{figure}[H]
\centering
\includegraphics[width=0.65\textwidth]{Layer-4.png}
\caption{Layer 4 visualization: Deep layer exhibits polysemanticity, mixing color, texture, and spatial cues.}
\end{figure}

\textbf{Interpretation:} 
With only 16 channels in Layer 4 but 10 digit classes $\times$ 10 colors $\times$ shape variations to encode, channels become polysemantic (responding to multiple unrelated features). 

\textbf{Information bottleneck theory} predicts this behavior: when representation dimensions fall below the information content, neurons must multiplex features. Layer 2, Channel 9, for example, simultaneously activates for cyan color (digit 5) and vertical line structures (digits 1, 7).

\subsubsection{Cross-Layer Summary}

\textbf{Key Finding:} Color-based spurious correlations \textbf{dominate throughout the entire network hierarchy}, not just early layers. This demonstrates that shortcut learning operates at the \textbf{representation level}, not merely at the decision boundary. The absence of shape-focused internal representations provides a \textbf{mechanistic explanation} for why the model catastrophically fails (24.3\%) when the color-label correlation is inverted on the hard test set.

\subsection{Interpretation}

The activation patterns indicate that \textbf{color information dominates internal representations}, even beyond the earliest layers. Despite sufficient capacity to learn shape-based features, the model instead develops \textbf{distributed, polysemantic representations aligned with the spurious color shortcut}.

This supports the hypothesis that under standard training (ERM), the model \textbf{lacks incentive to learn digit shape} when an easier predictive cue is available.

\section{Task 3: Gradient-Weighted Class Activation Mapping}

\subsection{Mathematical Foundation}

\textbf{Grad-CAM Formula:}

Given:
\begin{itemize}
    \item $A^k \in \mathbb{R}^{H \times W}$: Activation map of channel $k$ at target convolutional layer
    \item $y^c$: Score for class $c$ (before softmax)
\end{itemize}

Compute:
\begin{equation}
\alpha_k^c = \frac{1}{H \cdot W} \sum_{i,j} \frac{\partial y^c}{\partial A^k_{ij}}
\end{equation}

This is the \textbf{global average pooling} of gradients and represents how important channel $k$ is for predicting class $c$.

Then:
\begin{equation}
L^c_{\text{Grad-CAM}} = \text{ReLU}\left( \sum_k \alpha_k^c A^k \right)
\end{equation}

\textbf{Why ReLU?} Only features that have a \textbf{positive influence} on the class score are retained.

\subsection{Implementation}

Grad-CAM is implemented from scratch using forward and backward hooks on the final convolutional layer. During the forward pass, feature maps are stored; during the backward pass, gradients of the predicted class logit with respect to these feature maps are captured.

The heatmap is computed by:
(1) applying global average pooling to gradients to obtain channel-wise weights,
(2) computing a weighted sum of feature maps,
(3) applying ReLU,
(4) normalizing to $[0,1]$, and
(5) upsampling to input resolution using bilinear interpolation.

\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{shit.png}
\caption{Grad-CAM implementation visualization}
\end{figure}

\subsection{Results and Interpretation}

Grad-CAM visualizations provide insight into which regions of the input most influence the modelâ€™s prediction.

For \textbf{biased inputs} (e.g., digit `0` rendered in its dominant color), heatmaps frequently spread across colored digit regions and sometimes extend into the background, rather than tightly localizing to digit boundaries. This indicates that color cues strongly influence the prediction.

For \textbf{conflicting inputs} (e.g., digit `0` rendered in a non-dominant color), heatmaps become more diffuse and unstable. In several cases, attention shifts away from digit structure entirely, reflecting increased uncertainty and reliance on non-causal cues.

\subsection{Few Illustrations}

\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{SameGradCam.png}
\caption{Grad-CAM visualization for a biased input (`0` with dominant color)}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{DifferentGradCam.png}
\caption{Grad-CAM visualization for a conflicting input (`0` with random color)}
\end{figure}

\subsection{Discussion}

\subsubsection{What Grad-CAM Reveals}

\textbf{Sensitivity vs. Localization:}
\begin{itemize}
    \item Grad-CAM highlights regions to which the model's prediction is most \textit{sensitive}, not necessarily the object boundaries
    \item Different inputs of the same class yield different heatmaps when the model relies on spurious correlations
    \item This variability itself is diagnostic of shortcut learning
\end{itemize}

\subsubsection{Implementation Validation}

\textbf{Comparison with Library Implementations:}
\begin{itemize}
    \item Most visualizations align closely with \texttt{pytorch-grad-cam} library outputs
    \item Minor differences arise from:
    \begin{itemize}
        \item Layer selection strategy
        \item Normalization approach
        \item Interpolation method (bilinear vs. bicubic)
    \end{itemize}
    \item These variations do not indicate incorrect implementation --- Grad-CAM explanations are not unique
\end{itemize}

\subsubsection{Key Takeaway}

\begin{center}
\fbox{\parbox{0.9\textwidth}{%
\centering
\textbf{The observed heatmaps provide mechanistic evidence that the model relies on spurious color cues rather than digit shape, motivating the debiasing interventions explored in Task 4.}
}}
\end{center}


\section{Task 4: Debiasing Interventions}

\subsection{Overview}

I implemented \textbf{two distinct debiasing strategies}:
\begin{enumerate}
    \item \textbf{Task 4a:} Gradient Penalty (Input Regularization)
    \item \textbf{Task 4b:} Counterfactual Consistency Training
\end{enumerate}

\subsection{Task 4a: Gradient Penalty Debiasing}

\subsubsection{Hypothesis}

If we penalize the model for having high gradients with respect to \textbf{color channels}, it should learn to ignore color and focus on shape.

\subsubsection{Inspiration}

This approach is inspired by \textit{Right for the Right Reasons} (Ross et al.), which penalizes sensitivity to undesired input features to encourage correct reasoning.

\subsubsection{Mathematical Formulation}

\textbf{Loss Function:}
\begin{equation}
\mathcal{L} = \mathcal{L}_{\text{CE}}(f(x), y) + \lambda \cdot \left\| \frac{\partial f_y(x)}{\partial x} \right\|^2
\end{equation}

Where:
\begin{itemize}
    \item $f_y(x)$: Logit for the correct class $y$
    \item $\frac{\partial f_y(x)}{\partial x}$: Gradient of correct-class score w.r.t. input pixels
    \item $\lambda$: Penalty weight (hyperparameter)
\end{itemize}

\subsubsection{Implementation}

The gradient penalty loss function computes the standard cross-entropy loss, then calculates the gradient of the correct-class logit with respect to input pixels using PyTorch's autograd. The squared L2 norm of these gradients is computed and added to the cross-entropy loss with weight $\lambda_{gp}$.

\subsubsection{Results}

\begin{table}[H]
\centering
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Metric} & \textbf{Value} & \textbf{Interpretation} \\ 
\midrule
Train Accuracy (Easy) & (>95\%) & Still learns biased data reasonably \\
Test Accuracy (Hard) & 73\% & Significant improvement via gradient suppression \\
\bottomrule
\end{tabular}
\caption{Gradient penalty method results}
\end{table}

\subsubsection{Training Behavior}

	With {~5 epochs:}
\begin{itemize}
    \item Hard-set accuracy $\approx$ 45\%
    \item Model partially suppressed the shortcut but did not fully recover shape-based reasoning
\end{itemize}

	With {20 epochs:}
\begin{itemize}
    \item Hard-set accuracy $\approx$ 73\%
    \item Indicates gradual re-learning of digit shape under gradient constraints
\end{itemize}

\subsubsection{Effect of Training Duration}

Shortcut suppression is not instantaneous. Penalizing gradients requires additional training time for the model to:
\begin{itemize}
    \item Unlearn the shortcut
    \item Discover alternative predictive features
\end{itemize}
Longer training was critical for success.

\subsubsection{Effect of $\lambda$ (Penalty Strength)}

\begin{itemize}
    \item $\lambda = 0.5$ gave the best observed trade-off
    \item Increasing or decreasing $\lambda$ led to accuracy dropping to $\sim$50\%
\end{itemize}

This indicates:
\begin{itemize}
    \item Too small $\lambda$ $\rightarrow$ shortcut not sufficiently suppressed
    \item Too large $\lambda$ $\rightarrow$ model capacity overly constrained
\end{itemize}

\subsubsection{Key Observations}

\begin{itemize}
    \item Gradient suppression alone can significantly improve robustness
    \item Scheduling (warm-up) and training length are crucial
    \item The method is sensitive to gradient scale and hyperparameters
    \item Shortcut reliance can be reduced without changing the dataset
\end{itemize}

\subsubsection{Limitations}

\begin{itemize}
    \item Penalizes all input sensitivity equally
    \item Requires careful tuning of $\lambda$ and training duration
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{Confusion_Matrix_v1.png}
\caption{Confusion matrix for gradient penalty model (Task 4a) on hard test set, showing improved performance over the biased baseline with 73\% accuracy.}
\end{figure}

\subsection{Task 4b: Counterfactual Consistency Training}

\subsubsection{Hypothesis}

If we \textbf{explicitly train the model to make the same prediction for an image regardless of its color}, it will learn to ignore color.

\subsubsection{Dataset Generation}

Created a \textbf{paired counterfactual dataset}:

For each training image $(x, y)$:
\begin{enumerate}
    \item Generate colored version $x_{\text{color}}$ with biased color (95\% rule)
    \item Generate counterfactual $x_{\text{cf}}$ with \textbf{different random color}
    \item Both images have the \textbf{same label} $y$
\end{enumerate}

\subsubsection{Loss Function}

\textbf{Consistency Loss:}
\begin{equation}
\mathcal{L} = \mathcal{L}_{\text{CE}}(f(x), y) + \lambda_{\text{cf}} \cdot \mathbb{E}_{x_{\text{cf}}}\left[ \|f(x) - f(x_{\text{cf}})\|^2 \right]
\end{equation}

Where:
\begin{itemize}
    \item $f(x)$: Logits for original image
    \item $f(x_{\text{cf}})$: Logits for counterfactual image
    \item $\lambda_{\text{cf}}$: Consistency weight
\end{itemize}

\textbf{Intuition:} If the model predicts the same logits for both $x$ and $x_{\text{cf}}$ (which differ only in color), it must be using \textbf{color-invariant features} (i.e., shape).

\subsubsection{Training Strategy}

The training strategy processes pairs of images (original and counterfactual) in each batch. For the first 2 warmup epochs, only cross-entropy loss is used. After warmup, the consistency loss (mean squared difference of logits) is added with weight $\lambda_{cf} = 0.5$. Training continues for 10 total epochs.

\subsubsection{Hyperparameter Sensitivity}

Counterfactual consistency training achieved strong generalization performance with $\lambda_{\text{cf}} = 0.5$ on the first run. Given the clear improvement over the biased baseline, extensive hyperparameter tuning was not pursued. Limited experimentation with nearby values confirmed that the method is relatively robust to moderate changes in $\lambda_{\text{cf}}$.

\textbf{Selected Model:} $\lambda_{\text{cf}} = 0.5$, saved as \texttt{Robust_Models_cnn3_24v2_85}

\subsubsection{Results}

\begin{table}[H]
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Model} & \textbf{Train Acc} & \textbf{Test Acc} & \textbf{Improvement} \\ 
\midrule
Biased (Cheater) & (>95\%) & 24.3\% & Baseline \\
Gradient Penalty (4a) & (>95\%) & 73\% & +48.7\% \\
\textbf{Counterfactual (4b)} & \textbf{(>95\%)} & \textbf{85.2\%} & \textbf{+60.9\%} \\
\bottomrule
\end{tabular}
\caption{Debiasing methods comparison}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{Confusion_Matrix_v2.png}
\caption{Confusion matrix for counterfactual consistency model (Task 4b) on hard test set, achieving 85.2\% accuracy with substantially improved diagonal concentration.}
\end{figure}

\subsection{Comparative Analysis: Counterfactual vs. Gradient Penalty}

\subsubsection{Success Mechanisms of Counterfactual Training (Task 4b)}

Counterfactual consistency training achieves 85.2\% hard test accuracy through three key mechanisms:

\begin{enumerate}
    \item \textbf{Explicit Evidence of Color-Invariance:} The model observes paired examples $(x_{\text{red}}, x_{\text{blue}}, y=3)$ where the same digit appears in different colors but always maps to the same label. This provides direct supervision that color is uninformative.
    
    \item \textbf{Direct Consistency Optimization:} The MSE loss between $f(x)$ and $f(x_{\text{cf}})$ directly penalizes color-dependent decisions. This explicitly enforces color-invariant representations at the feature level.
    
    \item \textbf{Implicit Multi-Environment Training:} By presenting images with diverse color distributions, the method implicitly implements Invariant Risk Minimization (IRM), where the model must find causal features that generalize across multiple ``environments'' (color settings).
\end{enumerate}

\subsubsection{Why Gradient Penalty (Task 4a) Achieves Lower Performance}

Gradient penalty achieves 73\% accuracy, which significantly outperforms the biased model (24.3\%) and demonstrates that input sensitivity penalties can induce meaningful debiasing. However, it still falls short of counterfactual training (85.2\%). This performance gap arises from:

\begin{enumerate}
    \item \textbf{Indirect Signal:} Penalizing input gradients is a proxy for color-invariance, but the model can circumvent it by using color information in deeper layers or through nonlinear transformations.
    
    \item \textbf{Absence of Counterfactual Evidence:} The biased training set never explicitly shows that different colors should yield identical predictions. Without this direct evidence, the regularization signal is weaker.
    
    \item \textbf{Competing Objectives:} With 95\% color bias, the cross-entropy loss strongly rewards color-based decisions, while the gradient penalty only weakly discourages them.
\end{enumerate}

\subsubsection{Key Insight: Explicit Constraints Outperform Regularization Under Strong Bias}

This comparison reveals a fundamental principle: \textbf{when spurious correlations are extremely strong (95\%), explicit invariance constraints (counterfactual pairs) are substantially more effective than implicit regularization}. The model requires direct evidence that the shortcut is incorrect, not merely a penalty for using it.

\section{Task 5: Adversarial Robustness Analysis}

\subsection{Objective}

Compare the adversarial robustness of the \textbf{biased model} vs. \textbf{robust model (v2)} using targeted PGD attacks.

\subsection{Threat Model}

\textbf{Attack Scenario:}
\begin{itemize}
    \item Take an image of digit ``7''
    \item Goal: Make model predict ``3'' with $>90\%$ confidence
    \item Constraint: $L_\infty$ perturbation bounded by $\epsilon$
\end{itemize}

\subsection{Projected Gradient Descent (PGD) Attack}

\subsubsection{Mathematical Formulation}

Given an input image $x$ and target class $t$, PGD generates an adversarial example $x'$ by iteratively solving:

\begin{equation}
x^{(0)} = x + \text{Uniform}(-\epsilon, \epsilon)
\end{equation}

\begin{equation}
x^{(i+1)} = \Pi_{\epsilon}(x^{(i)} + \alpha \cdot \text{sign}(\nabla_x \mathcal{L}(f(x^{(i)}), t)))
\end{equation}

Where:
\begin{itemize}
    \item $\Pi_{\epsilon}(\cdot)$: Projection operator ensuring $\|x' - x\|_\infty \leq \epsilon$
    \item $\mathcal{L}(\cdot, t)$: Cross-entropy loss for target class $t$
    \item $\alpha$: Step size (learning rate)
    \item $\text{sign}(\cdot)$: Sign function for gradient direction
\end{itemize}

\subsubsection{Implementation Details}

The attack starts from a random point within the $\epsilon$-ball around the original image. For each iteration, it computes the gradient of the cross-entropy loss with respect to the target class, takes a step in the direction that increases the target class probability (using the sign of the gradient), and projects back to ensure the perturbation stays within the $\epsilon$-ball and valid pixel range $[0, 255]$.

\textbf{Key Parameters:}
\begin{itemize}
    \item \textbf{Epsilon ($\epsilon$):} 12.75 $\approx$ 0.05 $\times$ 255 (task specification)
    \item \textbf{Alpha ($\alpha$):} 1.5 (step size)
    \item \textbf{Steps:} 10 iterations (biased model), 30 iterations (robust model)
\end{itemize}

\subsection{Experimental Results}

\subsubsection{Test Case: Digit ``7'' $\rightarrow$ Target ``3''}

\begin{table}[H]
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Model} & \textbf{Success} & \textbf{Target Conf.} & \textbf{Required $\epsilon$} \\ 
\midrule
Biased & YES & 99\% & sometimes even 5 (way below budget!) \\
Robust & PARTIAL & 67.0\% & (>20) (exceeds budget!) \\
\bottomrule
\end{tabular}
\caption{Attack success at $\epsilon = 12.75$}
\end{table}

\textbf{Key Observation:} At $\epsilon = 12.75$, robust model achieves only \textbf{67\% confidence} on target class -- \textbf{attack unsuccessful} within budget.

\subsubsection{Epsilon Sweep Experiment}

\begin{table}[H]
\centering
\begin{tabular}{@{}ccc@{}}
\toprule
\textbf{Epsilon} & \textbf{Biased Success Rate} & \textbf{Robust Success Rate} \\ 
\midrule
5.0 & 95\% & 9\% \\
7.5 & 98\% & 39\% \\
10.0 & 99\% & 64\% \\
\textbf{12.75} & \textbf{99\%} & \textbf{68\%} \\
15.0 & 99\% & 81\% \\
20.0 & 100\% & 90\% \\
\bottomrule
\end{tabular}
\caption{Attack success rate vs. epsilon (Success = target confidence $>90\%$)}
\end{table}

\textbf{Key Finding:} Robust model is \textbf{1.5$\times$ more robust} at task-specified $\epsilon = 12.75$.

\subsection{Conclusion}

This analysis demonstrates that debiasing interventions not only improve generalization under distribution shift but also enhance adversarial robustness. The counterfactual consistency model (Task 4b), which learns shape-based representations, exhibits substantially greater resistance to targeted PGD attacks compared to the biased model relying on color shortcuts.

\textbf{Key Insight:} Models that learn causal features (shape) rather than spurious correlations (color) develop more robust decision boundaries. This suggests that \textbf{debiasing and adversarial robustness are complementary objectives}---both benefit from learning representations that capture invariant, semantically meaningful features.

\section{Weird Findings}

\subsection{Vanilla MNIST Evaluation}

We additionally evaluated both the biased and retrained models on vanilla MNIST (converted to three channels). Both models achieve near-chance accuracy ($\sim$10\%), indicating a severe domain shift. This does not contradict the gains observed in Task 4, as our interventions specifically target spurious color correlations within the Colored-MNIST distribution and do not enforce invariance to background texture or dataset-level statistics. This highlights the distinction between \textbf{shortcut robustness} and \textbf{domain generalization}.

\vspace{0.5cm}

\end{center}

\end{document}
