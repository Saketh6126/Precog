{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c987a767",
   "metadata": {},
   "source": [
    "# ***Imports***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "682a36cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321a7295",
   "metadata": {},
   "source": [
    "# ***Model Definition***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c0ede551",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheaterCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=9, padding=4, stride=2),  # BIG kernel → global color\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 16, kernel_size=5, padding=2, stride=2),  # BIG kernel + stride → even more global color\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16*7*7, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.features(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329dde16",
   "metadata": {},
   "source": [
    "# ***Loading the model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7d4555e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model(model_class, model_path, device):\n",
    "    \"\"\"\n",
    "    Load model weights for Grad-CAM visualization.\n",
    "    \n",
    "    Args:\n",
    "        model_class: The model class to instantiate\n",
    "        model_path: Path to the .pth file\n",
    "        device: torch device (cuda/cpu)\n",
    "    \n",
    "    Returns:\n",
    "        model: Model in eval mode with requires_grad enabled for inputs\n",
    "    \"\"\"\n",
    "    model = model_class().to(device)\n",
    "    state_dict = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    \n",
    "    # Eval mode: disables dropout, batch norm\n",
    "    model.eval()\n",
    "    \n",
    "    # Keep requires_grad=True for input (needed for backprop in Grad-CAM)\n",
    "    # but freeze all model parameters\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1480c8",
   "metadata": {},
   "source": [
    "# ***Color Palette & Coloring Functions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d53ec2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Palette with custom colors\n",
    "PALETTE = [\n",
    "    [255, 0, 0],     # 0: Red\n",
    "    [0, 255, 0],     # 1: Green\n",
    "    [0, 0, 255],     # 2: Blue\n",
    "    [255, 255, 0],   # 3: Yellow\n",
    "    [255, 0, 255],   # 4: Magenta\n",
    "    [0, 255, 255],   # 5: Cyan\n",
    "    [255, 128, 0],   # 6: Orange\n",
    "    [128, 0, 255],   # 7: Violet\n",
    "    [139, 69, 19],   # 8: Brown\n",
    "    [19, 139, 69]    # 9: Forest Green\n",
    "]\n",
    "BIAS_PROB = 0.95\n",
    "\n",
    "def select_biased_color(label, is_hard_set):\n",
    "    \"\"\"Determines color based on 95/5 rule or inversion for Hard Set.\"\"\"\n",
    "    if is_hard_set:\n",
    "        # Never use the shortcut color\n",
    "        wrong_colors = [c for i, c in enumerate(PALETTE) if i != label]\n",
    "        return random.choice(wrong_colors)\n",
    "    else:\n",
    "        # 95% shortcut, 5% random\n",
    "        if random.random() < BIAS_PROB:\n",
    "            return PALETTE[label]\n",
    "        else:\n",
    "            wrong_colors = [c for i, c in enumerate(PALETTE) if i != label]\n",
    "            return random.choice(wrong_colors)\n",
    "\n",
    "def color_foreground_stroke(grey_img, color_rgb):\n",
    "    \"\"\"\n",
    "    grey_img: (28, 28) uint8 MNIST digit\n",
    "    color_rgb: np.array([R, G, B]) in [0,255]\n",
    "\n",
    "    Returns:\n",
    "        (28, 28, 3) uint8 colored MNIST image\n",
    "    \"\"\"\n",
    "    # normalize digit\n",
    "    digit = grey_img.astype(np.float32) / 255.0\n",
    "\n",
    "    # grayscale background (uninformative)\n",
    "    bg = np.random.uniform(0.3, 0.6, (28, 28, 1))\n",
    "    bg = np.repeat(bg, 3, axis=2)\n",
    "\n",
    "    # normalize color\n",
    "    color = np.array(color_rgb, dtype=np.float32) / 255.0\n",
    "\n",
    "    # foreground stroke coloring (CRITICAL LINE)\n",
    "    img = bg * (1 - digit[..., None]) + digit[..., None] * color\n",
    "\n",
    "    # small noise to avoid pixel-perfect cues\n",
    "    img += np.random.randn(28, 28, 3) * 0.02\n",
    "    img = np.clip(img, 0, 1)\n",
    "\n",
    "    return (img * 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9a259b",
   "metadata": {},
   "source": [
    "# ***Loading and Colouring funcs***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9572819c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_digit_from_dataset(digit_value, dataset_path='Class', train=False):\n",
    "    \"\"\"\n",
    "    Get a random occurrence of a specific digit from .npy files in the 'Class' folder.\n",
    "    \n",
    "    Args:\n",
    "        digit_value: The digit to find (0-9)\n",
    "        dataset_path: Path to folder containing mnist_class_0.npy, ..., mnist_class_9.npy\n",
    "        train: Included for compatibility (not used, since data is pre-split)\n",
    "    \n",
    "    Returns:\n",
    "        digit: Grayscale image as numpy array (28, 28)\n",
    "        label: The digit label\n",
    "    \"\"\"\n",
    "    filename = os.path.join(dataset_path, f'mnist_class_{digit_value}.npy')\n",
    "    \n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"File {filename} not found.\")\n",
    "        return None, None\n",
    "\n",
    "    images = np.load(filename)  # Shape: (50, 28, 28)\n",
    "    idx = np.random.randint(0, images.shape[0])  # Pick random index\n",
    "    return images[idx], digit_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "62af5d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_colored_digit(digit, label, is_hard_set=True):\n",
    "    \"\"\"\n",
    "    Create a colored digit image using the biased color selection.\n",
    "    \n",
    "    Args:\n",
    "        digit: Grayscale MNIST digit (28, 28) numpy array\n",
    "        label: Ground truth label (0-9)\n",
    "        is_hard_set: If True, never use the shortcut color\n",
    "    \n",
    "    Returns:\n",
    "        colored_img: Colored digit as numpy array (28, 28, 3)\n",
    "        img_tensor: PyTorch tensor ready for model input (1, 3, 28, 28)\n",
    "    \"\"\"\n",
    "    # Select color and create colored image\n",
    "    color = select_biased_color(label, is_hard_set)\n",
    "    print(f\"Colour:{color}\")\n",
    "    # color = PALETTE[7]\n",
    "    colored_img = color_foreground_stroke(digit, color)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    img_tensor = torch.from_numpy(colored_img).float()\n",
    "    img_tensor = img_tensor.permute(2, 0, 1).unsqueeze(0)  # (1, 3, 28, 28)\n",
    "    \n",
    "    return colored_img, img_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36b956f",
   "metadata": {},
   "source": [
    "# ***PGD***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6b2a332b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def targeted_pgd(\n",
    "    model,\n",
    "    x,\n",
    "    target_label,\n",
    "    epsilon, # 0.05 * 255\n",
    "    alpha,\n",
    "    steps\n",
    "):\n",
    "    \"\"\"\n",
    "    Targeted PGD attack.\n",
    "    \n",
    "    Args:\n",
    "        model: trained classifier (outputs logits)\n",
    "        x: input image tensor (1, C, H, W)\n",
    "        target_label: int (desired wrong class)\n",
    "        epsilon: l_infinity budget\n",
    "        alpha: step size\n",
    "        steps: number of PGD iterations\n",
    "        \n",
    "    Returns:\n",
    "        x_adv: adversarial image\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Clone original image\n",
    "    x_orig = x.detach()\n",
    "    \n",
    "    # Random start within epsilon-ball (optional but recommended)\n",
    "    x_adv = x_orig + torch.empty_like(x_orig).uniform_(-epsilon, epsilon)\n",
    "    x_adv = torch.clamp(x_adv, 0.0, 255.0)\n",
    "\n",
    "    for _ in range(steps):\n",
    "        x_adv.requires_grad_(True)\n",
    "\n",
    "        logits = model(x_adv)\n",
    "\n",
    "        # Targeted loss: minimize CE toward target\n",
    "        loss = F.cross_entropy(logits, torch.tensor([target_label], device=x.device))\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Take step *toward* target class\n",
    "        x_adv = x_adv - alpha * x_adv.grad.sign()\n",
    "\n",
    "        # Project back to epsilon-ball\n",
    "        x_adv = torch.max(\n",
    "            torch.min(x_adv, x_orig + epsilon),\n",
    "            x_orig - epsilon\n",
    "        )\n",
    "\n",
    "        # Keep valid image range\n",
    "        x_adv = torch.clamp(x_adv, 0.0, 255.0)\n",
    "\n",
    "        x_adv = x_adv.detach()\n",
    "\n",
    "    return x_adv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d57e376",
   "metadata": {},
   "source": [
    "# ***Usage Examples***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b8e35ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Biased Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 3\n",
      "Target confidence: 0.9985673427581787\n",
      "\n",
      "\n",
      "\n",
      "Testing Robust Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 3\n",
      "Target confidence: 0.8000275492668152\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load model (update path to your model)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "Biased_Models = \"Models\"\n",
    "biased_model_path = os.path.join(Biased_Models, \"cheater_cnn3_24.pth\")\n",
    "\n",
    "Robust_Models = \"Robust_Models\"\n",
    "robust_model_path = os.path.join(Robust_Models, \"cnn3_24_v2_85.pth\")\n",
    "\n",
    "# Import your model class (e.g., from previous cells)\n",
    "biased_model = load_model(CheaterCNN, biased_model_path, device)\n",
    "robust_model = load_model(CheaterCNN, robust_model_path, device)\n",
    "\n",
    "# Comment this part if u want to fix the image and vary the hyperparams\n",
    "# Get digit from dataset\n",
    "# digit, label = get_digit_from_dataset(7)\n",
    "# # Color the digit\n",
    "# colored_img, img_tensor = create_colored_digit(digit, label, is_hard_set=False)\n",
    "# plt.imshow(colored_img)\n",
    "# plt.axis('off')\n",
    "# plt.show()\n",
    "\n",
    "# x = img_tensor.to(device)\n",
    "target_label = 3\n",
    "\n",
    "print(\"Testing Biased Model\")\n",
    "x_adv_biased = targeted_pgd(\n",
    "    model=biased_model,\n",
    "    x=x,\n",
    "    epsilon = 7.5,\n",
    "    alpha=1.5,\n",
    "    target_label=target_label,\n",
    "    steps=10\n",
    ")\n",
    "\n",
    "img = x_adv_biased.detach().cpu()[0]      # (3, 28, 28)\n",
    "img = img.permute(1, 2, 0)             # (28, 28, 3)\n",
    "img = img.numpy()\n",
    "plt.imshow(img.astype(np.uint8))\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = biased_model(x_adv_biased)\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "\n",
    "print(\"Predicted class:\", probs.argmax(dim=1).item())\n",
    "print(\"Target confidence:\", probs[0, target_label].item())\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"Testing Robust Model\")\n",
    "x_adv_robust = targeted_pgd(\n",
    "    model=robust_model,\n",
    "    x=x,\n",
    "    epsilon = 20,\n",
    "    alpha=1.5,\n",
    "    target_label=target_label,\n",
    "    steps=10\n",
    ")\n",
    "\n",
    "img = x_adv_robust.detach().cpu()[0]      # (3, 28, 28)\n",
    "img = img.permute(1, 2, 0)             # (28, 28, 3)\n",
    "img = img.numpy()\n",
    "plt.imshow(img.astype(np.uint8))\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = robust_model(x_adv_robust)\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "\n",
    "print(\"Predicted class:\", probs.argmax(dim=1).item())\n",
    "print(\"Target confidence:\", probs[0, target_label].item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
